{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty Estimation using TensorFlow Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.1.0-dev20191014\n",
      "TensorFlow Probability version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow_probability.python.layers import DenseVariational, DenseReparameterization, DenseFlipout, Convolution2DFlipout, Convolution2DReparameterization\n",
    "from tensorflow_probability.python.layers import DistributionLambda\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, BatchNormalization, Activation, LeakyReLU\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers import *\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "experimental_run_tf_function=False\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "print('TensorFlow Probability version:', tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "config = tf.compat.v1.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3 # GPU memory\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the dataset for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(n, w0, b0, x_low, x_high):\n",
    "    def s(x):\n",
    "        g = (x - x_low) / (x_high - x_low)\n",
    "        return 3 * (0.25 + g**2)\n",
    "    def f(x, w, b):\n",
    "        return w * x * (1. + np.sin(x)) + b\n",
    "    x = (x_high - x_low) * np.random.rand(n) + x_low  # N(x_low, x_high)\n",
    "    x = np.sort(x)\n",
    "    eps = np.random.randn(n) * s(x)\n",
    "    y = f(x, w0, b0) + eps\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_data = 500\n",
    "n_train = 400\n",
    "w0 = 0.125\n",
    "b0 = 5.0\n",
    "x_low, x_high = -20, 60\n",
    "\n",
    "X, y = load_dataset(n_data, w0, b0, x_low, x_high)\n",
    "X = np.expand_dims(X, 1)\n",
    "y = np.expand_dims(y, 1)\n",
    "\n",
    "idx_randperm = np.random.permutation(n_data)\n",
    "idx_train = np.sort(idx_randperm[:n_train])\n",
    "idx_test = np.sort(idx_randperm[n_train:])\n",
    "\n",
    "X_train, y_train = X[idx_train], y[idx_train]\n",
    "X_test, y_test = X[idx_test],y[idx_test]\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "\n",
    "plt.scatter(X_train, y_train, marker='+', label='Training data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Noisy training data and ground truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0],y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional point-estimate neural network\n",
    "## Define the loss function of negative log-likelihood (input as prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def neg_log_likelihood_with_dist(y_true, y_pred):\n",
    "    return -tf.reduce_mean(y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_epochs = 3000\n",
    "lr = 5e-3\n",
    "\n",
    "def build_point_estimate_model(scale=1):\n",
    "    model_in = Input(shape=(1,))\n",
    "    x = Dense(16)(model_in)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(1)(x)\n",
    "    model_out = DistributionLambda(lambda t: tfd.Normal(loc=t, scale=scale))(x)\n",
    "    model = Model(model_in, model_out)\n",
    "    return model\n",
    "\n",
    "pe_model = build_point_estimate_model()\n",
    "pe_model.compile(loss=neg_log_likelihood_with_dist, optimizer=Adam(lr), metrics=['mse'])\n",
    "pe_model.summary()\n",
    "hist = pe_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, \n",
    "                    verbose=1,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training loss and predict the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "ax[0].plot(range(n_epochs), hist.history['loss'],label='Training data')\n",
    "ax[0].plot(range(n_epochs), hist.history['val_loss'],label='Testing data')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training loss')\n",
    "ax[1].plot(range(n_epochs), hist.history['mean_squared_error'],label='Training data')\n",
    "ax[1].plot(range(n_epochs), hist.history['val_mean_squared_error'],label='Testing data')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean squared error')\n",
    "y_test_pred_pe = pe_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_pe.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='+', label='Training data')\n",
    "plt.scatter(X_test, y_test, marker='*', label='Training data')\n",
    "plt.plot(X_test, y_test_pred_pe.mean(), 'r-', marker='+', label='Test data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Noisy training data and ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate aleatoric uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_aleatoric_model():\n",
    "    model_in = Input(shape=(1,))\n",
    "    x = Dense(16)(model_in)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = Dense(16)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    model_out_loc = Dense(1)(x)\n",
    "    model_out_scale = Dense(1)(x)\n",
    "    model_out = DistributionLambda(\n",
    "        lambda t: tfd.Normal(loc=t[0],scale=1e-7 + \n",
    "                             tf.math.softplus(1e-3 * t[1])))([model_out_loc,model_out_scale])\n",
    "    model = Model(model_in, model_out)\n",
    "    return model\n",
    "\n",
    "al_model = build_aleatoric_model()\n",
    "al_model.compile(loss=neg_log_likelihood_with_dist, optimizer=Adam(lr), metrics=['mse'])\n",
    "al_model.summary()\n",
    "hist = al_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, \n",
    "                    verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "ax[0].plot(range(n_epochs), hist.history['loss'],label='Training data')\n",
    "ax[0].plot(range(n_epochs), hist.history['val_loss'],label='Testing data')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training loss')\n",
    "ax[1].plot(range(n_epochs), hist.history['mean_squared_error'])\n",
    "ax[1].plot(range(n_epochs), hist.history['val_mean_squared_error'])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean squared error')\n",
    "y_test_pred_al = al_model(X_test)\n",
    "y_test_pred_al_mean = y_test_pred_al.mean()\n",
    "y_test_pred_al_stddev = y_test_pred_al.stddev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='+', label='Training data')\n",
    "plt.plot(X_test, y_test_pred_al_mean, 'r-', marker='+', label='Test data')\n",
    "plt.fill_between(np.squeeze(X_test), \n",
    "                 np.squeeze(y_test_pred_al_mean + 2 * y_test_pred_al_stddev),\n",
    "                 np.squeeze(y_test_pred_al_mean - 2 * y_test_pred_al_stddev),\n",
    "                 alpha=0.5, label='Aleatoric uncertainty')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Noisy training data and ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate epistemic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "lr = 5e-3\n",
    "n_test = 10\n",
    "def posterior_mean_field(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    c = np.log(np.expm1(1.0))\n",
    "    return Sequential([tfp.layers.VariableLayer(2 * n, dtype=dtype),\n",
    "                       tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "                           tfd.Normal(loc=t[..., :n], scale=1e-7 + tf.nn.softplus(c + t[..., n:])),\n",
    "                           reinterpreted_batch_ndims=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_trainable(kernel_size, bias_size=0, dtype=None):\n",
    "    n = kernel_size + bias_size\n",
    "    return Sequential([tfp.layers.VariableLayer(n, dtype=dtype),\n",
    "                       tfp.layers.DistributionLambda(lambda t: tfd.Independent(\n",
    "                           tfd.Normal(loc=t, scale=1.0), reinterpreted_batch_ndims=1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_epistemic_model(train_size, scale=1):\n",
    "    model_in = Input(shape=(1,))\n",
    "    x = DenseVariational(16, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(model_in)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = DenseVariational(64, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = DenseVariational(16, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    model_out = DistributionLambda(lambda t: tfd.Normal(loc=t, scale=scale))(x)\n",
    "    model = Model(model_in, model_out)\n",
    "    return model\n",
    "\n",
    "ep_model = build_epistemic_model(n_train)\n",
    "ep_model.compile(loss=neg_log_likelihood_with_dist, optimizer=Adam(lr), metrics=['mse'])\n",
    "ep_model.summary()\n",
    "hist = ep_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1,\n",
    "                             validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({l.name: l.weights for l in ep_model.layers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "ax[0].plot(range(n_epochs), hist.history['loss'])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training loss')\n",
    "ax[0].set_yscale('log')\n",
    "ax[1].plot(range(n_epochs), hist.history['mean_squared_error'])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean squared error')\n",
    "ax[1].set_yscale('log')\n",
    "y_test_pred_ep_list = [ep_model(X_test) for _ in range(15)]\n",
    "y_test_pred_ep = ep_model(X_test)\n",
    "y_test_pred_ep_mean = y_test_pred_ep.mean()\n",
    "y_test_pred_ep_stddev = y_test_pred_ep.stddev()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test_pred_ep.shape\n",
    "y_test_pred_ep_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='+', label='Training data')\n",
    "plt.plot(X_test, y_test_pred_ep_mean, 'r-', marker='+', label='Test data')\n",
    "plt.fill_between(np.squeeze(X_test), \n",
    "                 np.squeeze(y_test_pred_ep_mean + 2 * y_test_pred_ep_stddev),\n",
    "                 np.squeeze(y_test_pred_ep_mean - 2 * y_test_pred_ep_stddev),\n",
    "                 alpha=0.5, label='Epistemic uncertainty')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Noisy training data and ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='+', label='Training data')\n",
    "avg_mean = np.zeros_like(X_test)\n",
    "for i, y in enumerate(y_test_pred_ep_list):\n",
    "    y_mean = y.mean()\n",
    "    plt.plot(X_test, y_mean, 'r-', marker='+', label='Ensemble tests' if i == 0 else None, linewidth=0.5)\n",
    "    avg_mean += y_mean\n",
    "plt.plot(X_test, avg_mean/n_test, 'g-', marker='+', label='Averaged tests', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Noisy training data and ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate aleatoric + epistemic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30000\n",
    "def build_aleatoric_epistemic_model(train_size):\n",
    "    model_in = Input(shape=(1,))\n",
    "    x = DenseVariational(16, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(model_in)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = DenseVariational(64, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    x = DenseVariational(16, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    x = LeakyReLU(0.1)(x)\n",
    "    model_out_loc = DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    model_out_scale = DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/train_size)(x)\n",
    "    model_out = DistributionLambda(lambda t: tfd.Normal(loc=t[0],\n",
    "                            scale=1e-7 + tf.math.softplus(1e-3 * t[1])))([model_out_loc,model_out_scale])\n",
    "    model = Model(model_in, model_out)\n",
    "    return model\n",
    "\n",
    "ae_model = build_aleatoric_epistemic_model(n_train)\n",
    "ae_model.compile(loss=neg_log_likelihood_with_dist, optimizer=Adam(lr), metrics=['mse'])\n",
    "ae_model.summary()\n",
    "hist = ae_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1,\n",
    "                   validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({l.name: l.weights for l in ae_model.layers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(15, 10))\n",
    "ax[0].plot(range(n_epochs), hist.history['loss'])\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Training loss')\n",
    "ax[0].set_yscale('log')\n",
    "ax[1].plot(range(n_epochs), hist.history['mean_squared_error'])\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('Mean squared error')\n",
    "ax[1].set_yscale('log')\n",
    "y_test_pred_ae_list = [ae_model(X_test) for _ in range(n_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, marker='+', label='Training data')\n",
    "avg_mean = np.zeros_like(X_test)\n",
    "for i, y in enumerate(y_test_pred_ae_list):\n",
    "    y_mean = y.mean()\n",
    "    y_stddev = y.stddev()\n",
    "    plt.plot(X_test, y_mean, 'r-', marker='+', label='Ensemble tests' if i == 0 else None, linewidth=0.5)\n",
    "    plt.fill_between(np.squeeze(X_test), \n",
    "                     np.squeeze(y_mean + 2 * y_stddev),\n",
    "                     np.squeeze(y_mean - 2 * y_stddev),\n",
    "                     alpha=0.5, label='Aleatoric uncertainties' if i == 0 else None)\n",
    "    avg_mean += y_mean\n",
    "plt.plot(X_test, avg_mean/n_test, 'g-', marker='+', label='Averaged tests', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Noisy training data and ground truth')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainties in CNNs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neg_log_likelihood_with_logits(y_true, y_pred):\n",
    "    y_pred_dist = tfp.distributions.Categorical(logits=y_pred)\n",
    "    return -tf.reduce_mean(y_pred_dist.log_prob(tf.argmax(y_true, axis=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset\n",
      "X_train.shape = (60000, 28, 28, 1)\n",
      "y_train.shape = (60000, 10)\n",
      "X_test.shape = (10000, 28, 28, 1)\n",
      "y_test.shape = (10000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f40640fb150>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_class = 10\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 20\n",
    "lr = 1e-3\n",
    "\n",
    "print('Loading MNIST dataset')\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, -1)\n",
    "n_train = X_train.shape[0]\n",
    "X_test = np.expand_dims(X_test, -1)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, n_class)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, n_class)\n",
    "\n",
    "# Normalize data\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "print(\"X_train.shape =\", X_train.shape)\n",
    "print(\"y_train.shape =\", y_train.shape)\n",
    "print(\"X_test.shape =\", X_test.shape)\n",
    "print(\"y_test.shape =\", y_test.shape)\n",
    "\n",
    "plt.imshow(X_train[0, :, :, 0], cmap='gist_gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel_divergence_fn(train_size, w=1.0):\n",
    "    \"\"\"\n",
    "    Get the kernel Kullback-Leibler divergence function\n",
    "\n",
    "    # Arguments\n",
    "        train_size (int): size of the training dataset for normalization\n",
    "        w (float): weight to the function\n",
    "\n",
    "    # Returns\n",
    "        kernel_divergence_fn: kernel Kullback-Leibler divergence function\n",
    "    \"\"\"\n",
    "    def kernel_divergence_fn(q, p, _):  # need the third ignorable argument\n",
    "        kernel_divergence = tfp.distributions.kl_divergence(q, p) / tf.cast(train_size, tf.float32)\n",
    "        return w * kernel_divergence\n",
    "    return kernel_divergence_fn\n",
    "\n",
    "def add_kl_weight(layer, train_size, w_value=1.0):\n",
    "    w = layer.add_weight(name=layer.name+'/kl_loss_weight', shape=(),\n",
    "                         initializer=tf.initializers.constant(w_value), trainable=False)\n",
    "    layer.kernel_divergence_fn = get_kernel_divergence_fn(train_size, w)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/xyu/anaconda3/envs/tensorflow_gpu/lib/python3.7/site-packages/tensorflow_probability/python/layers/util.py:104: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout (Conv2DFlipou (None, 14, 14, 32)        609       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_flipout_1 (Conv2DFlip (None, 7, 7, 64)          36929     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_flipout (DenseFlipout) (None, 512)               3211777   \n",
      "_________________________________________________________________\n",
      "dense_flipout_1 (DenseFlipou (None, 10)                10251     \n",
      "=================================================================\n",
      "Total params: 3,259,950\n",
      "Trainable params: 3,259,754\n",
      "Non-trainable params: 196\n",
      "_________________________________________________________________\n",
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 9s 171us/sample - loss: 67.5502 - acc: 0.8401 - val_loss: 64.7431 - val_acc: 0.9333\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 7s 130us/sample - loss: 61.9963 - acc: 0.9405 - val_loss: 59.0669 - val_acc: 0.9568\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 7s 124us/sample - loss: 55.9929 - acc: 0.9590 - val_loss: 52.8229 - val_acc: 0.9680\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 7s 124us/sample - loss: 49.6268 - acc: 0.9676 - val_loss: 46.3941 - val_acc: 0.9733\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 7s 130us/sample - loss: 43.2062 - acc: 0.9720 - val_loss: 40.0445 - val_acc: 0.9737\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 7s 127us/sample - loss: 36.9969 - acc: 0.9761 - val_loss: 34.0238 - val_acc: 0.9788\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 7s 123us/sample - loss: 31.2475 - acc: 0.9756 - val_loss: 28.5875 - val_acc: 0.9758\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 7s 131us/sample - loss: 26.1676 - acc: 0.9757 - val_loss: 23.8741 - val_acc: 0.9770\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 7s 131us/sample - loss: 21.8400 - acc: 0.9784 - val_loss: 19.9388 - val_acc: 0.9788\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 7s 127us/sample - loss: 18.2591 - acc: 0.9791 - val_loss: 16.7164 - val_acc: 0.9773\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 15.3515 - acc: 0.9794 - val_loss: 14.1105 - val_acc: 0.9790\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 7s 128us/sample - loss: 13.0001 - acc: 0.9801 - val_loss: 11.9951 - val_acc: 0.9777\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 7s 130us/sample - loss: 11.1000 - acc: 0.9798 - val_loss: 10.2955 - val_acc: 0.9780\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 7s 131us/sample - loss: 9.5615 - acc: 0.9799 - val_loss: 8.9054 - val_acc: 0.9788\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 7s 124us/sample - loss: 8.2939 - acc: 0.9804 - val_loss: 7.7483 - val_acc: 0.9810\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 7s 125us/sample - loss: 7.2424 - acc: 0.9796 - val_loss: 6.7849 - val_acc: 0.9782\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 7s 129us/sample - loss: 6.3535 - acc: 0.9806 - val_loss: 5.9763 - val_acc: 0.9783\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 5.6010 - acc: 0.9809 - val_loss: 5.2849 - val_acc: 0.9788\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 7s 126us/sample - loss: 4.9687 - acc: 0.9798 - val_loss: 4.6950 - val_acc: 0.9797\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 7s 127us/sample - loss: 4.4246 - acc: 0.9809 - val_loss: 4.1988 - val_acc: 0.9798\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_bayesian_bcnn_model(input_shape, train_size):\n",
    "    model_in = Input(shape=input_shape)\n",
    "    conv_1 = Convolution2DFlipout(32, kernel_size=(3, 3), padding=\"same\", strides=2,\n",
    "                                  kernel_divergence_fn=None)\n",
    "    conv_1 = add_kl_weight(conv_1, train_size)\n",
    "    x = conv_1(model_in)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    conv_2 = Convolution2DFlipout(64, kernel_size=(3, 3), padding=\"same\", strides=2,\n",
    "                                  kernel_divergence_fn=None)\n",
    "    conv_2 = add_kl_weight(conv_2, train_size)\n",
    "    x = conv_2(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Flatten()(x)\n",
    "    dense_1 = DenseFlipout(512, activation='relu',\n",
    "                           kernel_divergence_fn=None)\n",
    "    dense_1 = add_kl_weight(dense_1, train_size)\n",
    "    x = dense_1(x)\n",
    "    dense_2 = DenseFlipout(10, activation=None,\n",
    "                           kernel_divergence_fn=None)\n",
    "    dense_2 = add_kl_weight(dense_2, train_size)\n",
    "    model_out = dense_2(x)  # logits\n",
    "    model = Model(model_in, model_out)\n",
    "    return model\n",
    "    \n",
    "bcnn_model = build_bayesian_bcnn_model(X_train.shape[1:], n_train)\n",
    "bcnn_model.compile(loss=neg_log_likelihood_with_logits, optimizer=Adam(lr), metrics=['acc'],\n",
    "                   experimental_run_tf_function=False)\n",
    "bcnn_model.summary()\n",
    "hist = bcnn_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({l.name: l.weights for l in bcnn_model.layers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mc_run = 100\n",
    "med_prob_thres = 0.2\n",
    "ff = [bcnn_model.predict(X_test[0:1],verbose=1)for _ in range(n_mc_run)]\n",
    "ff_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] \n",
    "                                  for y in ff], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_pred_prob_all = np.concatenate([softmax(f,axis=-1) for f in ff])\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in ff_pred_prob_all]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logits_list = [bcnn_model.predict(X_test,verbose=1) for _ in range(n_mc_run)]  # a list of predicted logits\n",
    "y_pred_prob_all = np.concatenate([softmax(y, axis=-1)[:, :, np.newaxis] for y in y_pred_logits_list], axis=-1)\n",
    "y_pred = [[int(np.median(y) >= med_prob_thres) for y in y_pred_prob] for y_pred_prob in y_pred_prob_all]\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "idx_valid = [any(y) for y in y_pred]\n",
    "print('Number of recognizable samples:', sum(idx_valid))\n",
    "\n",
    "idx_invalid = [not any(y) for y in y_pred]\n",
    "print('Unrecognizable samples:', np.where(idx_invalid)[0])\n",
    "\n",
    "print('Test accuracy on MNIST (recognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_valid], axis=-1), np.argmax(y_pred[idx_valid], axis=-1))) / len(y_test[idx_valid]))\n",
    "\n",
    "print('Test accuracy on MNIST (unrecognizable samples):',\n",
    "      sum(np.equal(np.argmax(y_test[idx_invalid], axis=-1), np.argmax(y_pred[idx_invalid], axis=-1))) / len(y_test[idx_invalid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred_hist(y_pred, n_class, n_mc_run, n_bins=30, med_prob_thres=0.2, n_subplot_rows=2, figsize=(25, 10)):\n",
    "    bins = np.logspace(-n_bins, 0, n_bins+1)\n",
    "    fig, ax = plt.subplots(n_subplot_rows, n_class // n_subplot_rows + 1, figsize=figsize)\n",
    "    for i in range(n_subplot_rows):\n",
    "        for j in range(n_class // n_subplot_rows + 1):\n",
    "            idx = i * (n_class // n_subplot_rows + 1) + j\n",
    "            if idx < n_class:\n",
    "                ax[i, j].hist(y_pred[idx], bins)\n",
    "                ax[i, j].set_xscale('log')\n",
    "                ax[i, j].set_ylim([0, n_mc_run])\n",
    "                ax[i, j].title.set_text(\"{} (median prob: {:.2f}) ({})\".format(str(idx),\n",
    "                                                                               np.median(y_pred[idx]),\n",
    "                                                                               str(np.median(y_pred[idx]) >= med_prob_thres)))\n",
    "            else:\n",
    "                ax[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 149\n",
    "plt.imshow(X_test[idx, :, :, 0], cmap='gist_gray')\n",
    "print(\"True label of the test sample {}: {}\".format(idx, np.argmax(y_test[idx], axis=-1)))\n",
    "\n",
    "plot_pred_hist(y_pred_prob_all[idx], n_class, n_mc_run, med_prob_thres=med_prob_thres)\n",
    "\n",
    "if any(y_pred[idx]):\n",
    "    print(\"Predicted label of the test sample {}: {}\".format(idx, np.argmax(y_pred[idx], axis=-1)))\n",
    "else:\n",
    "    print(\"I don't know!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243.47px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "468.86px",
    "left": "463.39px",
    "right": "20px",
    "top": "128px",
    "width": "659px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
